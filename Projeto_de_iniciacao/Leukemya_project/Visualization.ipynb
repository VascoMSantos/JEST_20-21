{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Samples\n",
    "df = pd.read_csv('Leukemya_data.csv', header=None)\n",
    "\n",
    "#Labels\n",
    "labels = pd.read_csv('labels.csv', header=None)\n",
    "y = labels.add(-1) #transformar as labels em 0 e 1\n",
    "\n",
    "#Separar as 128 samples com labels das 50 samples sem labels\n",
    "predict_data = df.iloc[128:,:]\n",
    "X = df.iloc[:128,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    111\n1     17\nName: 0, dtype: int64"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#Proproção entre 0 e 1 no dataset\n",
    "y[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              0           1           2           3           4           5    \\\ncount  128.000000  128.000000  128.000000  128.000000  128.000000  128.000000   \nmean     0.598454    0.199827   -0.314046   -0.279318    0.628314    0.285455   \nstd      0.072532    0.031099    0.598357    1.070608    0.100354    0.090456   \nmin      0.376770    0.103500   -1.299800   -1.472100    0.338550    0.088063   \n25%      0.555222    0.190150   -0.725690   -0.983997    0.585620    0.218935   \n50%      0.609730    0.203310   -0.457110   -0.549370    0.652150    0.277400   \n75%      0.652520    0.217172   -0.050245   -0.068488    0.701812    0.349807   \nmax      0.725140    0.264250    1.878300    5.181600    0.790610    0.511740   \n\n              6           7           8           9    ...         176  \\\ncount  128.000000  128.000000  128.000000  128.000000  ...  128.000000   \nmean     0.551004    0.082255   -0.790809    0.175583  ...    0.464284   \nstd      0.036781    0.015182    0.659477    1.326650  ...    0.631497   \nmin      0.424760    0.029278   -2.059300   -1.617800  ...   -1.979500   \n25%      0.531698    0.075788   -1.256475   -0.784435  ...    0.109180   \n50%      0.560330    0.086056   -0.879090   -0.177425  ...    0.513780   \n75%      0.575120    0.091499   -0.486663    0.866645  ...    0.831695   \nmax      0.615770    0.108670    1.514800    5.000000  ...    2.320500   \n\n              177         178         179         180         181         182  \\\ncount  128.000000  128.000000  128.000000  128.000000  128.000000  128.000000   \nmean     1.075097    0.333877    0.126951    0.226637    0.101913    1.517887   \nstd      1.247563    0.068351    0.039831    0.038796    0.030314    1.124836   \nmin     -1.331900    0.171240    0.059768    0.149200    0.015450   -0.298160   \n25%      0.294132    0.297960    0.099242    0.205205    0.090534    0.850782   \n50%      0.838475    0.328520    0.119270    0.220830    0.106930    1.407550   \n75%      1.596775    0.373015    0.145382    0.248677    0.122075    1.854850   \nmax      6.687300    0.604310    0.316170    0.334130    0.156020    6.031600   \n\n              183         184         185  \ncount  128.000000  128.000000  128.000000  \nmean     3.429096    0.185997    0.132880  \nstd      9.670882    0.044779    0.083610  \nmin     -1.734000    0.140280    0.000000  \n25%     -0.813325    0.163120    0.062720  \n50%      0.761485    0.173315    0.107145  \n75%      2.469725    0.183520    0.205440  \nmax     66.292000    0.359070    0.317360  \n\n[8 rows x 186 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>176</th>\n      <th>177</th>\n      <th>178</th>\n      <th>179</th>\n      <th>180</th>\n      <th>181</th>\n      <th>182</th>\n      <th>183</th>\n      <th>184</th>\n      <th>185</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>...</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n      <td>128.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.598454</td>\n      <td>0.199827</td>\n      <td>-0.314046</td>\n      <td>-0.279318</td>\n      <td>0.628314</td>\n      <td>0.285455</td>\n      <td>0.551004</td>\n      <td>0.082255</td>\n      <td>-0.790809</td>\n      <td>0.175583</td>\n      <td>...</td>\n      <td>0.464284</td>\n      <td>1.075097</td>\n      <td>0.333877</td>\n      <td>0.126951</td>\n      <td>0.226637</td>\n      <td>0.101913</td>\n      <td>1.517887</td>\n      <td>3.429096</td>\n      <td>0.185997</td>\n      <td>0.132880</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.072532</td>\n      <td>0.031099</td>\n      <td>0.598357</td>\n      <td>1.070608</td>\n      <td>0.100354</td>\n      <td>0.090456</td>\n      <td>0.036781</td>\n      <td>0.015182</td>\n      <td>0.659477</td>\n      <td>1.326650</td>\n      <td>...</td>\n      <td>0.631497</td>\n      <td>1.247563</td>\n      <td>0.068351</td>\n      <td>0.039831</td>\n      <td>0.038796</td>\n      <td>0.030314</td>\n      <td>1.124836</td>\n      <td>9.670882</td>\n      <td>0.044779</td>\n      <td>0.083610</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.376770</td>\n      <td>0.103500</td>\n      <td>-1.299800</td>\n      <td>-1.472100</td>\n      <td>0.338550</td>\n      <td>0.088063</td>\n      <td>0.424760</td>\n      <td>0.029278</td>\n      <td>-2.059300</td>\n      <td>-1.617800</td>\n      <td>...</td>\n      <td>-1.979500</td>\n      <td>-1.331900</td>\n      <td>0.171240</td>\n      <td>0.059768</td>\n      <td>0.149200</td>\n      <td>0.015450</td>\n      <td>-0.298160</td>\n      <td>-1.734000</td>\n      <td>0.140280</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.555222</td>\n      <td>0.190150</td>\n      <td>-0.725690</td>\n      <td>-0.983997</td>\n      <td>0.585620</td>\n      <td>0.218935</td>\n      <td>0.531698</td>\n      <td>0.075788</td>\n      <td>-1.256475</td>\n      <td>-0.784435</td>\n      <td>...</td>\n      <td>0.109180</td>\n      <td>0.294132</td>\n      <td>0.297960</td>\n      <td>0.099242</td>\n      <td>0.205205</td>\n      <td>0.090534</td>\n      <td>0.850782</td>\n      <td>-0.813325</td>\n      <td>0.163120</td>\n      <td>0.062720</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.609730</td>\n      <td>0.203310</td>\n      <td>-0.457110</td>\n      <td>-0.549370</td>\n      <td>0.652150</td>\n      <td>0.277400</td>\n      <td>0.560330</td>\n      <td>0.086056</td>\n      <td>-0.879090</td>\n      <td>-0.177425</td>\n      <td>...</td>\n      <td>0.513780</td>\n      <td>0.838475</td>\n      <td>0.328520</td>\n      <td>0.119270</td>\n      <td>0.220830</td>\n      <td>0.106930</td>\n      <td>1.407550</td>\n      <td>0.761485</td>\n      <td>0.173315</td>\n      <td>0.107145</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.652520</td>\n      <td>0.217172</td>\n      <td>-0.050245</td>\n      <td>-0.068488</td>\n      <td>0.701812</td>\n      <td>0.349807</td>\n      <td>0.575120</td>\n      <td>0.091499</td>\n      <td>-0.486663</td>\n      <td>0.866645</td>\n      <td>...</td>\n      <td>0.831695</td>\n      <td>1.596775</td>\n      <td>0.373015</td>\n      <td>0.145382</td>\n      <td>0.248677</td>\n      <td>0.122075</td>\n      <td>1.854850</td>\n      <td>2.469725</td>\n      <td>0.183520</td>\n      <td>0.205440</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.725140</td>\n      <td>0.264250</td>\n      <td>1.878300</td>\n      <td>5.181600</td>\n      <td>0.790610</td>\n      <td>0.511740</td>\n      <td>0.615770</td>\n      <td>0.108670</td>\n      <td>1.514800</td>\n      <td>5.000000</td>\n      <td>...</td>\n      <td>2.320500</td>\n      <td>6.687300</td>\n      <td>0.604310</td>\n      <td>0.316170</td>\n      <td>0.334130</td>\n      <td>0.156020</td>\n      <td>6.031600</td>\n      <td>66.292000</td>\n      <td>0.359070</td>\n      <td>0.317360</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 186 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#Pequena descrição do dataset\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    111\n1     17\nName: 0, dtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#De modo a obter um dataset equilibrado - Fazer oversampling, uma vez que que temos um dataset pequeno\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE(random_state=42)\n",
    "\n",
    "X2, y2 = oversample.fit_resample(X, y)\n",
    "y[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((222, 186), (222, 1))"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((155, 186), (67, 186), (155, 1), (67, 1))"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#Dividir os dados em dados de treino e dados de teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "\n",
    "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0         1         2         3         4         5         6    \\\n0    1.000000  0.571570 -0.928204 -0.535481  0.974411  0.329458  0.708757   \n1    0.571570  1.000000 -0.710771 -0.814761  0.609479  0.886626  0.707371   \n2   -0.928204 -0.710771  1.000000  0.635170 -0.940009 -0.458372 -0.861759   \n3   -0.535481 -0.814761  0.635170  1.000000 -0.529580 -0.836795 -0.555862   \n4    0.974411  0.609479 -0.940009 -0.529580  1.000000  0.380068  0.758156   \n..        ...       ...       ...       ...       ...       ...       ...   \n181  0.289174  0.570656 -0.499443 -0.384363  0.392681  0.465671  0.636138   \n182 -0.386378 -0.601743  0.545737  0.453267 -0.465637 -0.516138 -0.671206   \n183 -0.403812 -0.630893  0.551787  0.509076 -0.457613 -0.507626 -0.709683   \n184  0.160388  0.264356 -0.238240 -0.225057  0.244898  0.327651  0.274931   \n185  0.271315  0.415240 -0.398812 -0.319937  0.349145  0.407962  0.428265   \n\n          7         8         9    ...       176       177       178  \\\n0    0.401059 -0.837738 -0.132661  ...  0.228076  0.023438 -0.081713   \n1    0.891925 -0.693096 -0.674308  ...  0.580379 -0.119694 -0.447415   \n2   -0.575958  0.947172  0.249458  ... -0.386276  0.002005  0.244725   \n3   -0.713709  0.535970  0.739658  ... -0.325112  0.096962  0.190115   \n4    0.458012 -0.866209 -0.167338  ...  0.320091  0.042051 -0.180708   \n..        ...       ...       ...  ...       ...       ...       ...   \n181  0.686628 -0.577262 -0.306559  ...  0.733714  0.111652 -0.687271   \n182 -0.713090  0.569845  0.391167  ... -0.717674 -0.148028  0.668407   \n183 -0.738448  0.540806  0.449369  ... -0.616439 -0.150406  0.553564   \n184  0.316916 -0.271785 -0.165745  ...  0.443935  0.163565 -0.400888   \n185  0.498820 -0.454051 -0.223005  ...  0.616504  0.129249 -0.584903   \n\n          179       180       181       182       183       184       185  \n0   -0.128570  0.279088  0.289174 -0.386378 -0.403812  0.160388  0.271315  \n1    0.257366  0.462209  0.570656 -0.601743 -0.630893  0.264356  0.415240  \n2    0.123500 -0.442353 -0.499443  0.545737  0.551787 -0.238240 -0.398812  \n3   -0.231204 -0.335546 -0.384363  0.453267  0.509076 -0.225057 -0.319937  \n4   -0.171285  0.378827  0.392681 -0.465637 -0.457613  0.244898  0.349145  \n..        ...       ...       ...       ...       ...       ...       ...  \n181 -0.168433  0.919330  1.000000 -0.872208 -0.781533  0.607627  0.851097  \n182  0.154256 -0.883032 -0.872208  1.000000  0.922815 -0.656170 -0.813553  \n183  0.133465 -0.703473 -0.781533  0.922815  1.000000 -0.442128 -0.606927  \n184 -0.161010  0.848694  0.607627 -0.656170 -0.442128  1.000000  0.749291  \n185 -0.154729  0.934658  0.851097 -0.813553 -0.606927  0.749291  1.000000  \n\n[186 rows x 186 columns]\n"
    }
   ],
   "source": [
    "corr = Xtrain.corr() #Matriz de correlação\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Correlação entre 0 e 4\nCorrelação entre 2 e 8\nCorrelação entre 6 e 10\nCorrelação entre 10 e 40\nCorrelação entre 12 e 16\nCorrelação entre 18 e 22\nCorrelação entre 24 e 28\nCorrelação entre 24 e 66\nCorrelação entre 24 e 70\nCorrelação entre 25 e 67\nCorrelação entre 26 e 68\nCorrelação entre 26 e 128\nCorrelação entre 28 e 66\nCorrelação entre 28 e 70\nCorrelação entre 29 e 71\nCorrelação entre 30 e 34\nCorrelação entre 36 e 40\nCorrelação entre 42 e 46\nCorrelação entre 48 e 52\nCorrelação entre 54 e 58\nCorrelação entre 60 e 64\nCorrelação entre 66 e 70\nCorrelação entre 72 e 76\nCorrelação entre 78 e 82\nCorrelação entre 79 e 83\nCorrelação entre 84 e 88\nCorrelação entre 85 e 89\nCorrelação entre 90 e 94\nCorrelação entre 91 e 95\nCorrelação entre 96 e 100\nCorrelação entre 102 e 106\nCorrelação entre 104 e 105\nCorrelação entre 108 e 112\nCorrelação entre 108 e 113\nCorrelação entre 110 e 111\nCorrelação entre 116 e 117\nCorrelação entre 120 e 121\nCorrelação entre 120 e 124\nCorrelação entre 120 e 125\nCorrelação entre 121 e 125\nCorrelação entre 122 e 123\nCorrelação entre 126 e 130\nCorrelação entre 132 e 136\nCorrelação entre 133 e 137\nCorrelação entre 139 e 157\nCorrelação entre 140 e 141\nCorrelação entre 144 e 148\nCorrelação entre 146 e 147\nCorrelação entre 148 e 149\nCorrelação entre 150 e 154\nCorrelação entre 152 e 153\nCorrelação entre 158 e 159\nCorrelação entre 162 e 166\nCorrelação entre 168 e 172\nCorrelação entre 174 e 178\nCorrelação entre 180 e 181\nCorrelação entre 180 e 185\nCorrelação entre 182 e 183\n"
    }
   ],
   "source": [
    "def feature_drop(df):\n",
    "    \"\"\"Esta função elimina certas colunas de um dataset\n",
    "    com base na sua matriz de correlação\"\"\"\n",
    "    global list\n",
    "    list=[]\n",
    "    Xtrain2 = df\n",
    "    for i in range(186):\n",
    "        for j in range(i, 186):\n",
    "            value = corr.loc[i].iat[j]\n",
    "            if value>0.9 and j!=i:\n",
    "                print(f'Correlação entre {i} e {j}')\n",
    "                if j not in list:\n",
    "                    list.append(j)\n",
    "\n",
    "    Xtrain2 = Xtrain2.drop(list,axis=1)\n",
    "    return Xtrain2\n",
    "\n",
    "Xtrain2 = feature_drop(Xtrain)\n",
    "Xtest2 = Xtest.drop(list,axis=1)\n",
    "predict_data2 = predict_data.drop(list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0         1         2         3         5         6         7    \\\n182  0.456121  0.125120  1.325341  3.024584  0.129743  0.487328  0.052456   \n135  0.516983  0.127788  0.903578  2.025405  0.141086  0.486965  0.047446   \n86   0.646320  0.197520 -0.789460  0.076750  0.218200  0.575950  0.078682   \n65   0.666770  0.187610 -0.918730  0.462410  0.203520  0.577310  0.075230   \n207  0.664382  0.194045 -0.592446 -0.062782  0.249574  0.537223  0.073950   \n..        ...       ...       ...       ...       ...       ...       ...   \n106  0.617480  0.214380 -0.573730 -0.633010  0.287670  0.577640  0.089599   \n14   0.622580  0.215920 -0.584100 -0.530650  0.277890  0.568990  0.084487   \n92   0.725140  0.198860 -1.156900  0.635540  0.205480  0.594220  0.078664   \n179  0.583320  0.146183  0.284645  0.229740  0.181588  0.497442  0.046786   \n102  0.673770  0.197870 -1.046400  0.433920  0.196670  0.572060  0.079362   \n\n          9         11        12   ...       171       173       174  \\\n182  1.517226  0.055944  0.613929  ...  2.359284  0.098970  0.454454   \n135  1.560570  0.051395  0.556035  ...  7.398438  0.054606  0.385813   \n86   0.899230  0.058637  0.546790  ...  0.412810  0.075408  0.398300   \n65   2.347700  0.047963  0.514130  ...  0.143510  0.099957  0.332810   \n207  0.774251  0.076983  0.603659  ... -0.023542  0.121741  0.464572   \n..        ...       ...       ...  ...       ...       ...       ...   \n106 -0.303220  0.119570  0.528360  ... -0.655820  0.159490  0.374400   \n14   0.406880  0.070319  0.519760  ... -0.273730  0.129430  0.388160   \n92   1.518400  0.049561  0.653880  ... -0.233610  0.151170  0.358910   \n179  1.362502  0.052348  0.521454  ...  6.579383  0.076159  0.403102   \n102  1.205100  0.049758  0.517170  ... -0.500180  0.121360  0.349370   \n\n          175       176       177       179       180       182       184  \n182  0.096893 -1.187531  1.835709  0.101493  0.168765  3.846218  0.153779  \n135  0.091323 -0.712674  0.524339  0.108461  0.165602  3.401321  0.153936  \n86   0.110980 -0.101150  0.874130  0.110890  0.196110  1.926100  0.160500  \n65   0.100640  0.569450  0.406670  0.113540  0.205430  1.521700  0.163120  \n207  0.126428 -0.686019  0.251491  0.150212  0.163147  3.466096  0.149029  \n..        ...       ...       ...       ...       ...       ...       ...  \n106  0.111910  0.186320  0.287530  0.120540  0.218210  1.691000  0.172270  \n14   0.117050 -0.220900 -0.072788  0.131920  0.231230  1.500100  0.179880  \n92   0.101180  0.276670  0.968420  0.111460  0.287200  0.490770  0.208120  \n179  0.095070 -0.590679  0.442113  0.122618  0.163470  3.461385  0.154591  \n102  0.095799  0.279240  1.350600  0.099015  0.220740  1.392800  0.175980  \n\n[155 rows x 133 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>9</th>\n      <th>11</th>\n      <th>12</th>\n      <th>...</th>\n      <th>171</th>\n      <th>173</th>\n      <th>174</th>\n      <th>175</th>\n      <th>176</th>\n      <th>177</th>\n      <th>179</th>\n      <th>180</th>\n      <th>182</th>\n      <th>184</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>182</th>\n      <td>0.456121</td>\n      <td>0.125120</td>\n      <td>1.325341</td>\n      <td>3.024584</td>\n      <td>0.129743</td>\n      <td>0.487328</td>\n      <td>0.052456</td>\n      <td>1.517226</td>\n      <td>0.055944</td>\n      <td>0.613929</td>\n      <td>...</td>\n      <td>2.359284</td>\n      <td>0.098970</td>\n      <td>0.454454</td>\n      <td>0.096893</td>\n      <td>-1.187531</td>\n      <td>1.835709</td>\n      <td>0.101493</td>\n      <td>0.168765</td>\n      <td>3.846218</td>\n      <td>0.153779</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>0.516983</td>\n      <td>0.127788</td>\n      <td>0.903578</td>\n      <td>2.025405</td>\n      <td>0.141086</td>\n      <td>0.486965</td>\n      <td>0.047446</td>\n      <td>1.560570</td>\n      <td>0.051395</td>\n      <td>0.556035</td>\n      <td>...</td>\n      <td>7.398438</td>\n      <td>0.054606</td>\n      <td>0.385813</td>\n      <td>0.091323</td>\n      <td>-0.712674</td>\n      <td>0.524339</td>\n      <td>0.108461</td>\n      <td>0.165602</td>\n      <td>3.401321</td>\n      <td>0.153936</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>0.646320</td>\n      <td>0.197520</td>\n      <td>-0.789460</td>\n      <td>0.076750</td>\n      <td>0.218200</td>\n      <td>0.575950</td>\n      <td>0.078682</td>\n      <td>0.899230</td>\n      <td>0.058637</td>\n      <td>0.546790</td>\n      <td>...</td>\n      <td>0.412810</td>\n      <td>0.075408</td>\n      <td>0.398300</td>\n      <td>0.110980</td>\n      <td>-0.101150</td>\n      <td>0.874130</td>\n      <td>0.110890</td>\n      <td>0.196110</td>\n      <td>1.926100</td>\n      <td>0.160500</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0.666770</td>\n      <td>0.187610</td>\n      <td>-0.918730</td>\n      <td>0.462410</td>\n      <td>0.203520</td>\n      <td>0.577310</td>\n      <td>0.075230</td>\n      <td>2.347700</td>\n      <td>0.047963</td>\n      <td>0.514130</td>\n      <td>...</td>\n      <td>0.143510</td>\n      <td>0.099957</td>\n      <td>0.332810</td>\n      <td>0.100640</td>\n      <td>0.569450</td>\n      <td>0.406670</td>\n      <td>0.113540</td>\n      <td>0.205430</td>\n      <td>1.521700</td>\n      <td>0.163120</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.664382</td>\n      <td>0.194045</td>\n      <td>-0.592446</td>\n      <td>-0.062782</td>\n      <td>0.249574</td>\n      <td>0.537223</td>\n      <td>0.073950</td>\n      <td>0.774251</td>\n      <td>0.076983</td>\n      <td>0.603659</td>\n      <td>...</td>\n      <td>-0.023542</td>\n      <td>0.121741</td>\n      <td>0.464572</td>\n      <td>0.126428</td>\n      <td>-0.686019</td>\n      <td>0.251491</td>\n      <td>0.150212</td>\n      <td>0.163147</td>\n      <td>3.466096</td>\n      <td>0.149029</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>0.617480</td>\n      <td>0.214380</td>\n      <td>-0.573730</td>\n      <td>-0.633010</td>\n      <td>0.287670</td>\n      <td>0.577640</td>\n      <td>0.089599</td>\n      <td>-0.303220</td>\n      <td>0.119570</td>\n      <td>0.528360</td>\n      <td>...</td>\n      <td>-0.655820</td>\n      <td>0.159490</td>\n      <td>0.374400</td>\n      <td>0.111910</td>\n      <td>0.186320</td>\n      <td>0.287530</td>\n      <td>0.120540</td>\n      <td>0.218210</td>\n      <td>1.691000</td>\n      <td>0.172270</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.622580</td>\n      <td>0.215920</td>\n      <td>-0.584100</td>\n      <td>-0.530650</td>\n      <td>0.277890</td>\n      <td>0.568990</td>\n      <td>0.084487</td>\n      <td>0.406880</td>\n      <td>0.070319</td>\n      <td>0.519760</td>\n      <td>...</td>\n      <td>-0.273730</td>\n      <td>0.129430</td>\n      <td>0.388160</td>\n      <td>0.117050</td>\n      <td>-0.220900</td>\n      <td>-0.072788</td>\n      <td>0.131920</td>\n      <td>0.231230</td>\n      <td>1.500100</td>\n      <td>0.179880</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>0.725140</td>\n      <td>0.198860</td>\n      <td>-1.156900</td>\n      <td>0.635540</td>\n      <td>0.205480</td>\n      <td>0.594220</td>\n      <td>0.078664</td>\n      <td>1.518400</td>\n      <td>0.049561</td>\n      <td>0.653880</td>\n      <td>...</td>\n      <td>-0.233610</td>\n      <td>0.151170</td>\n      <td>0.358910</td>\n      <td>0.101180</td>\n      <td>0.276670</td>\n      <td>0.968420</td>\n      <td>0.111460</td>\n      <td>0.287200</td>\n      <td>0.490770</td>\n      <td>0.208120</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>0.583320</td>\n      <td>0.146183</td>\n      <td>0.284645</td>\n      <td>0.229740</td>\n      <td>0.181588</td>\n      <td>0.497442</td>\n      <td>0.046786</td>\n      <td>1.362502</td>\n      <td>0.052348</td>\n      <td>0.521454</td>\n      <td>...</td>\n      <td>6.579383</td>\n      <td>0.076159</td>\n      <td>0.403102</td>\n      <td>0.095070</td>\n      <td>-0.590679</td>\n      <td>0.442113</td>\n      <td>0.122618</td>\n      <td>0.163470</td>\n      <td>3.461385</td>\n      <td>0.154591</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>0.673770</td>\n      <td>0.197870</td>\n      <td>-1.046400</td>\n      <td>0.433920</td>\n      <td>0.196670</td>\n      <td>0.572060</td>\n      <td>0.079362</td>\n      <td>1.205100</td>\n      <td>0.049758</td>\n      <td>0.517170</td>\n      <td>...</td>\n      <td>-0.500180</td>\n      <td>0.121360</td>\n      <td>0.349370</td>\n      <td>0.095799</td>\n      <td>0.279240</td>\n      <td>1.350600</td>\n      <td>0.099015</td>\n      <td>0.220740</td>\n      <td>1.392800</td>\n      <td>0.175980</td>\n    </tr>\n  </tbody>\n</table>\n<p>155 rows × 133 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "Xtrain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0         1         2         3         5         6         7    \\\n154  0.520785  0.182583  0.233224 -0.453144  0.271421  0.529132  0.071441   \n215  0.566613  0.133363  0.110959  0.547993  0.159650  0.472884  0.042155   \n15   0.550020  0.240310 -0.010231 -1.456400  0.446180  0.539660  0.090968   \n173  0.478191  0.156702  1.094282  1.532995  0.157164  0.482215  0.072827   \n158  0.530019  0.133420  0.539395  0.742605  0.159850  0.487301  0.045643   \n..        ...       ...       ...       ...       ...       ...       ...   \n198  0.587516  0.143918  0.265428  0.306523  0.177426  0.500745  0.051824   \n206  0.476294  0.152546  1.039258  1.495883  0.143706  0.484604  0.065577   \n113  0.657080  0.196940 -0.953000  0.167970  0.194720  0.566410  0.079175   \n5    0.592830  0.239070 -0.429870 -1.086400  0.408020  0.562480  0.090431   \n56   0.674590  0.204670 -0.875280 -0.076688  0.230920  0.571910  0.086981   \n\n          9         11        12   ...       171       173       174  \\\n154 -0.304470  0.116314  0.547261  ... -0.444863  0.193442  0.384009   \n215  2.491550  0.051316  0.451933  ...  1.069653  0.078536  0.545247   \n15  -0.824740  0.149900  0.488440  ... -1.515900  0.273980  0.282010   \n173  0.465817  0.078710  0.544307  ... -1.254861  0.265313  0.297690   \n158  1.914177  0.048853  0.591275  ...  1.310251  0.117574  0.470702   \n..        ...       ...       ...  ...       ...       ...       ...   \n198  0.991619  0.065863  0.511718  ...  8.341588  0.050906  0.397216   \n206  0.883693  0.063087  0.537090  ... -0.814128  0.241584  0.317502   \n113  0.996760  0.051613  0.533290  ... -0.253110  0.113630  0.365760   \n5   -0.888550  0.157350  0.525090  ... -1.155100  0.264340  0.363880   \n56   0.471130  0.063099  0.555390  ... -0.601110  0.161150  0.312800   \n\n          175       176       177       179       180       182       184  \n154  0.122378  0.112193 -0.012783  0.176526  0.208992  2.216455  0.163368  \n215  0.090439 -1.707239  4.199853  0.077152  0.158971  2.290402  0.150015  \n15   0.105330  0.727430  0.294820  0.158160  0.281990  0.522950  0.179880  \n173  0.133197  0.894645  0.107229  0.189251  0.180541  2.432443  0.156387  \n158  0.102542 -1.126647  2.236755  0.113409  0.165756  4.725240  0.154305  \n..        ...       ...       ...       ...       ...       ...       ...  \n198  0.086311 -0.708833  1.325906  0.098556  0.167347  3.178013  0.156648  \n206  0.123463  0.677838  0.253009  0.171767  0.177702  2.900768  0.154850  \n113  0.103610 -0.061330  0.778270  0.108360  0.225290  1.223300  0.180450  \n5    0.128590  0.290770 -0.098939  0.165180  0.216160  1.678300  0.178740  \n56   0.097618  0.616060  1.074200  0.113510  0.270310  0.540620  0.194060  \n\n[67 rows x 133 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>9</th>\n      <th>11</th>\n      <th>12</th>\n      <th>...</th>\n      <th>171</th>\n      <th>173</th>\n      <th>174</th>\n      <th>175</th>\n      <th>176</th>\n      <th>177</th>\n      <th>179</th>\n      <th>180</th>\n      <th>182</th>\n      <th>184</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>154</th>\n      <td>0.520785</td>\n      <td>0.182583</td>\n      <td>0.233224</td>\n      <td>-0.453144</td>\n      <td>0.271421</td>\n      <td>0.529132</td>\n      <td>0.071441</td>\n      <td>-0.304470</td>\n      <td>0.116314</td>\n      <td>0.547261</td>\n      <td>...</td>\n      <td>-0.444863</td>\n      <td>0.193442</td>\n      <td>0.384009</td>\n      <td>0.122378</td>\n      <td>0.112193</td>\n      <td>-0.012783</td>\n      <td>0.176526</td>\n      <td>0.208992</td>\n      <td>2.216455</td>\n      <td>0.163368</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>0.566613</td>\n      <td>0.133363</td>\n      <td>0.110959</td>\n      <td>0.547993</td>\n      <td>0.159650</td>\n      <td>0.472884</td>\n      <td>0.042155</td>\n      <td>2.491550</td>\n      <td>0.051316</td>\n      <td>0.451933</td>\n      <td>...</td>\n      <td>1.069653</td>\n      <td>0.078536</td>\n      <td>0.545247</td>\n      <td>0.090439</td>\n      <td>-1.707239</td>\n      <td>4.199853</td>\n      <td>0.077152</td>\n      <td>0.158971</td>\n      <td>2.290402</td>\n      <td>0.150015</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.550020</td>\n      <td>0.240310</td>\n      <td>-0.010231</td>\n      <td>-1.456400</td>\n      <td>0.446180</td>\n      <td>0.539660</td>\n      <td>0.090968</td>\n      <td>-0.824740</td>\n      <td>0.149900</td>\n      <td>0.488440</td>\n      <td>...</td>\n      <td>-1.515900</td>\n      <td>0.273980</td>\n      <td>0.282010</td>\n      <td>0.105330</td>\n      <td>0.727430</td>\n      <td>0.294820</td>\n      <td>0.158160</td>\n      <td>0.281990</td>\n      <td>0.522950</td>\n      <td>0.179880</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>0.478191</td>\n      <td>0.156702</td>\n      <td>1.094282</td>\n      <td>1.532995</td>\n      <td>0.157164</td>\n      <td>0.482215</td>\n      <td>0.072827</td>\n      <td>0.465817</td>\n      <td>0.078710</td>\n      <td>0.544307</td>\n      <td>...</td>\n      <td>-1.254861</td>\n      <td>0.265313</td>\n      <td>0.297690</td>\n      <td>0.133197</td>\n      <td>0.894645</td>\n      <td>0.107229</td>\n      <td>0.189251</td>\n      <td>0.180541</td>\n      <td>2.432443</td>\n      <td>0.156387</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>0.530019</td>\n      <td>0.133420</td>\n      <td>0.539395</td>\n      <td>0.742605</td>\n      <td>0.159850</td>\n      <td>0.487301</td>\n      <td>0.045643</td>\n      <td>1.914177</td>\n      <td>0.048853</td>\n      <td>0.591275</td>\n      <td>...</td>\n      <td>1.310251</td>\n      <td>0.117574</td>\n      <td>0.470702</td>\n      <td>0.102542</td>\n      <td>-1.126647</td>\n      <td>2.236755</td>\n      <td>0.113409</td>\n      <td>0.165756</td>\n      <td>4.725240</td>\n      <td>0.154305</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>0.587516</td>\n      <td>0.143918</td>\n      <td>0.265428</td>\n      <td>0.306523</td>\n      <td>0.177426</td>\n      <td>0.500745</td>\n      <td>0.051824</td>\n      <td>0.991619</td>\n      <td>0.065863</td>\n      <td>0.511718</td>\n      <td>...</td>\n      <td>8.341588</td>\n      <td>0.050906</td>\n      <td>0.397216</td>\n      <td>0.086311</td>\n      <td>-0.708833</td>\n      <td>1.325906</td>\n      <td>0.098556</td>\n      <td>0.167347</td>\n      <td>3.178013</td>\n      <td>0.156648</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.476294</td>\n      <td>0.152546</td>\n      <td>1.039258</td>\n      <td>1.495883</td>\n      <td>0.143706</td>\n      <td>0.484604</td>\n      <td>0.065577</td>\n      <td>0.883693</td>\n      <td>0.063087</td>\n      <td>0.537090</td>\n      <td>...</td>\n      <td>-0.814128</td>\n      <td>0.241584</td>\n      <td>0.317502</td>\n      <td>0.123463</td>\n      <td>0.677838</td>\n      <td>0.253009</td>\n      <td>0.171767</td>\n      <td>0.177702</td>\n      <td>2.900768</td>\n      <td>0.154850</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.657080</td>\n      <td>0.196940</td>\n      <td>-0.953000</td>\n      <td>0.167970</td>\n      <td>0.194720</td>\n      <td>0.566410</td>\n      <td>0.079175</td>\n      <td>0.996760</td>\n      <td>0.051613</td>\n      <td>0.533290</td>\n      <td>...</td>\n      <td>-0.253110</td>\n      <td>0.113630</td>\n      <td>0.365760</td>\n      <td>0.103610</td>\n      <td>-0.061330</td>\n      <td>0.778270</td>\n      <td>0.108360</td>\n      <td>0.225290</td>\n      <td>1.223300</td>\n      <td>0.180450</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.592830</td>\n      <td>0.239070</td>\n      <td>-0.429870</td>\n      <td>-1.086400</td>\n      <td>0.408020</td>\n      <td>0.562480</td>\n      <td>0.090431</td>\n      <td>-0.888550</td>\n      <td>0.157350</td>\n      <td>0.525090</td>\n      <td>...</td>\n      <td>-1.155100</td>\n      <td>0.264340</td>\n      <td>0.363880</td>\n      <td>0.128590</td>\n      <td>0.290770</td>\n      <td>-0.098939</td>\n      <td>0.165180</td>\n      <td>0.216160</td>\n      <td>1.678300</td>\n      <td>0.178740</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>0.674590</td>\n      <td>0.204670</td>\n      <td>-0.875280</td>\n      <td>-0.076688</td>\n      <td>0.230920</td>\n      <td>0.571910</td>\n      <td>0.086981</td>\n      <td>0.471130</td>\n      <td>0.063099</td>\n      <td>0.555390</td>\n      <td>...</td>\n      <td>-0.601110</td>\n      <td>0.161150</td>\n      <td>0.312800</td>\n      <td>0.097618</td>\n      <td>0.616060</td>\n      <td>1.074200</td>\n      <td>0.113510</td>\n      <td>0.270310</td>\n      <td>0.540620</td>\n      <td>0.194060</td>\n    </tr>\n  </tbody>\n</table>\n<p>67 rows × 133 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "Xtest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  1,   2,   5,   6,  11,  29,  39,  41,  43,  68,  69,  89,  90,\n        95,  96,  98, 125, 127, 130, 131], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#Reduzir o Xtrain2, o Xtest2 e o predict_data2 a apenas 20 features (20 \"melhores\" features)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "SKB = SelectKBest(score_func=f_classif, k=20)\n",
    "Xtrain_fselected = SKB.fit_transform(Xtrain2, ytrain)\n",
    "Xtest_fselected = SKB.transform(Xtest2)\n",
    "predict_data_fselected = SKB.transform(predict_data2)\n",
    "cols = SKB.get_support(indices=True)\n",
    "\n",
    "Xtrain_fselected = pd.DataFrame(data=Xtrain_fselected, columns=cols)\n",
    "Xtest_fselected = pd.DataFrame(data=Xtest_fselected, columns=cols)\n",
    "predict_data_fselected = pd.DataFrame(data=predict_data_fselected, columns=cols)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((155, 20), (67, 20), (50, 20))"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "Xtrain_fselected.shape, Xtest_fselected.shape, predict_data_fselected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def check_accuracy(y_pred,y_true):\n",
    "    n=0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i]==y_true[i]:\n",
    "            n=n+1\n",
    "    acc = n/len(y_true)\n",
    "    return acc\n",
    "\n",
    "def precision(pred,test):\n",
    "    true_positives = 0\n",
    "    positive_predictions = (pred == 1).sum()\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i] == test[i] and test[i] == 1:\n",
    "            true_positives+=1\n",
    "    return true_positives/positive_predictions\n",
    "    \n",
    "def recall(pred,test):\n",
    "    true_positives = 0\n",
    "    positives = (test == 1).sum()\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i] == test[i] and test[i] == 1:\n",
    "            true_positives+=1\n",
    "    return true_positives/positives\n",
    "\n",
    "def F1score(precision,recall):\n",
    "    score = 2*(precision*recall)/(precision+recall)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = ytest.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n       0], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#KKN - K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=2,p=2)\n",
    "clf_KNN.fit(Xtrain_fselected, ytrain)\n",
    "\n",
    "y_pred = clf_KNN.predict(Xtest_fselected)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "K Neighbors Classifier accuracy is:  0.9701492537313433\nK Neighbors Classifier precision is:  0.9696969696969697\nK Neighbors Classifier recall is:  0.9696969696969697\nK Neighbors Classifier F1 score is:  0.9696969696969697\n"
    }
   ],
   "source": [
    "#Medidas do desempenho\n",
    "ACC=check_accuracy(y_pred, ytest)\n",
    "PREC=precision(y_pred, ytest)\n",
    "REC=recall(y_pred, ytest)\n",
    "F1=F1score(PREC, REC)\n",
    "\n",
    "print('K Neighbors Classifier accuracy is: ', ACC)\n",
    "print('K Neighbors Classifier precision is: ', PREC)\n",
    "print('K Neighbors Classifier recall is: ', REC)\n",
    "print('K Neighbors Classifier F1 score is: ', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97        34\n           1       0.97      0.97      0.97        33\n\n    accuracy                           0.97        67\n   macro avg       0.97      0.97      0.97        67\nweighted avg       0.97      0.97      0.97        67\n\n"
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[33,  1],\n       [ 1, 32]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "#Matriz de confusão\n",
    "cmatrix = confusion_matrix(ytest, y_pred)\n",
    "cmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_DTC = DecisionTreeClassifier(random_state=42)\n",
    "clf_DTC.fit(Xtrain_fselected, ytrain)\n",
    "\n",
    "y_pred2 = clf_DTC.predict(Xtest_fselected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Decision Tree Classifier accuracy is:  0.9402985074626866\nDecision Tree Classifier precision is:  0.9393939393939394\nDecision Tree Classifier recall is:  0.9393939393939394\nDecision Tree Classifier F1 score is:  0.9393939393939394\n"
    }
   ],
   "source": [
    "#Medidas do desempenho\n",
    "ACC2=check_accuracy(y_pred2, ytest)\n",
    "PREC2=precision(y_pred2, ytest)\n",
    "REC2=recall(y_pred2, ytest)\n",
    "F12=F1score(PREC2, REC2)\n",
    "\n",
    "print('Decision Tree Classifier accuracy is: ', ACC2)\n",
    "print('Decision Tree Classifier precision is: ', PREC2)\n",
    "print('Decision Tree Classifier recall is: ', REC2)\n",
    "print('Decision Tree Classifier F1 score is: ', F12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94        34\n           1       0.94      0.94      0.94        33\n\n    accuracy                           0.94        67\n   macro avg       0.94      0.94      0.94        67\nweighted avg       0.94      0.94      0.94        67\n\n"
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(ytest, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[32,  2],\n       [ 2, 31]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "#Matriz de confusão\n",
    "cmatrix2 = confusion_matrix(ytest, y_pred2)\n",
    "cmatrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_RFC = RandomForestClassifier(random_state=42)\n",
    "clf_RFC.fit(Xtrain_fselected, ytrain)\n",
    "\n",
    "y_pred3 = clf_RFC.predict(Xtest_fselected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Random Forest Classifier accuracy is:  0.9701492537313433\nRandom Forest Classifier precision is:  0.9696969696969697\nRandom Forest Classifier recall is:  0.9696969696969697\nRandom Forest Classifier F1 score is:  0.9696969696969697\n"
    }
   ],
   "source": [
    "#Medidas do desempenho\n",
    "ACC3=check_accuracy(y_pred3, ytest)\n",
    "PREC3=precision(y_pred3, ytest)\n",
    "REC3=recall(y_pred3, ytest)\n",
    "F13=F1score(PREC3, REC3)\n",
    "\n",
    "print('Random Forest Classifier accuracy is: ', ACC3)\n",
    "print('Random Forest Classifier precision is: ', PREC3)\n",
    "print('Random Forest Classifier recall is: ', REC3)\n",
    "print('Random Forest Classifier F1 score is: ', F13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97        34\n           1       0.97      0.97      0.97        33\n\n    accuracy                           0.97        67\n   macro avg       0.97      0.97      0.97        67\nweighted avg       0.97      0.97      0.97        67\n\n"
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(ytest, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[33,  1],\n       [ 1, 32]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "#Matriz de confusão\n",
    "cmatrix3 = confusion_matrix(ytest, y_pred3)\n",
    "cmatrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste real dos três classificadores\n",
    "prediction_KNN = clf_KNN.predict(predict_data_fselected)\n",
    "prediction_DTC = clf_DTC.predict(predict_data_fselected)\n",
    "prediction_RFC = clf_RFC.predict(predict_data_fselected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(None, None, None)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "#Salvar os testes reais em ficheiros csv\n",
    "import csv\n",
    "import os\n",
    "ID = predict_data.index.values\n",
    "def save(ID, array, filename):\n",
    "    df = pd.DataFrame(data=array, index=ID, columns=['Predictions'])\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    df.to_csv(f'./results/{filename}')\n",
    "\n",
    "\n",
    "save(ID,prediction_DTC,'results_DTC.csv'), save(ID,prediction_KNN,'results_KNN.csv'), save(ID,prediction_RFC,'results_RFC.csv') "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}